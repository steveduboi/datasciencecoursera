---
title: Practical Machine Learning - Course Project
author: "steve d."
date: "10/20/2017"
---

### Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 
Mission
In this project, we will use this data -- from accelerometers on the belt, forearm, arm, and dumbell of 6 participant who asked to perform barbell lifts correctly and incorrectly in 5 different ways; the goal of this project, is to predict the manner in which they did these exercises.

###  Loading the necesary R packages.
```{r, echo = FALSE}
library(ggplot2)
library(rpart)
library(caret)
library(knitr)
library(randomForest)
````

#Download the data
#Read the training and testing datasets and replace empty values by NA
```{r, echo=TRUE}
trainingDataS<- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", sep=",", header=TRUE, na.strings = c("NA","",'#DIV/0!'))

testingDataS<- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", sep=",", header=TRUE, na.strings = c("NA","",'#DIV/0!'))
dim(trainingDataS)
```

Our data consists of 19622 values of 160 variables.

Clean the data by removing columns with missing values
```{r, echo=TRUE}
trainingDataS <- trainingDataS[,(colSums(is.na(trainingDataS)) == 0)]
testingDataS <- testingDataS[,(colSums(is.na(testingDataSet)) == 0)]
dim(trainingDataS)
dim(testingDataSet)
```
We reduced our data to 60 variables.

### Preprocess the data
```{r, echo=TRUE}
numIdx <- which(lapply(trainingDataS, class) %in% "numeric")
preprocessModel <-preProcess(trainingDataS[, numIdx],method=c('knnImpute', 'center', 'scale'))
prep_trainingDataS <- predict(preprocessModel, trainingDataS[,numericalsIdx])
prep_trainingDataS$classe <- trainingDataS$classe
pre_testingDataS <-predict(preprocessModel,testingDataSet[,numIdx])
```

###Removing the non zero variables
Removing the variables with values near zero, that means that they are not significant in the predictions
```{r, echo=TRUE}
nzv <- nearZeroVar(pre_trainingDataS,saveMetrics=TRUE)
pre_trainingDataS <- pre_trainingDataS[,nzv$nzv==FALSE]

nzv <- nearZeroVar(pre_testingDataS,saveMetrics=TRUE)
pre_testingDataS <- pre_testingDataS[,nzv$nzv==FALSE]
```
Validation set

We want a 75% observation training dataset to train our model. We will then validate it on the last 70%.

set.seed(12031987)
inTrain<- createDataPartition(pre_trainingDataSet$classe, p=3/4, list=FALSE)
training<- pre_trainingDataSet[inTrain, ]
validation <- pre_trainingDataSet[-inTrain, ]
dim(training) ; dim(validation)
## [1] 14718    28
## [1] 4904   28
Train Model
We train a model using random forest with a cross validation of 5 folds to avoid overfitting.

library(randomForest)
modFitrf <- train(classe ~., method="rf", data=training, trControl=trainControl(method='cv'), number=5, allowParallel=TRUE, importance=TRUE )
modFitrf
## Random Forest 
## 
## 14718 samples
##    27 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 13246, 13245, 13248, 13245, 13247, 13246, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
##    2    0.9927294  0.9908028  0.001700103  0.002150719
##   14    0.9927966  0.9908879  0.002836707  0.003588893
##   27    0.9889914  0.9860747  0.004497867  0.005690737
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 14.
Interpretation
Let’s plot the importance of each individual variable

# varImpPlot(modFitrf$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 0.6, main = "Importance of the Individual Principal Components")
This plot shows each of the principal components in order from most important to least important.

Cross Validation Testing and Out-of-Sample Error Estimate
Let’s apply our training model on our testing database, to check its accuracy.

Accuracy and Estimated out of sample error

predValidRF <- predict(modFitrf, validation)
confus <- confusionMatrix(validation$classe, predValidRF)
confus$table
##           Reference
## Prediction    A    B    C    D    E
##          A 1392    2    1    0    0
##          B    4  944    1    0    0
##          C    0    2  848    5    0
##          D    0    0    1  803    0
##          E    0    0    1    3  897
We can notice that there are very few variables out of this model.

accur <- postResample(validation$classe, predValidRF)
modAccuracy <- accur[[1]]
modAccuracy
## [1] 0.9959217
out_of_sample_error <- 1 - modAccuracy
out_of_sample_error
## [1] 0.004078303
The estimated accuracy of the model is 99.7% and the estimated out-of-sample error based on our fitted model applied to the cross validation dataset is 0.3%.

Application of this model on the 20 test cases provided
We have already clean the test data base (teData). We delete the “problem id” column as it is useless for our analysis.

pred_final <- predict(modFitrf, pre_testingDataSet)
pred_final
##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E



